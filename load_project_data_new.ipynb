{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Davies Corpora\n",
    "\n",
    "This notebook will explore the corpora in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #for http requests\n",
    "import pandas as pd#gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud #Makes word clouds\n",
    "import numpy as np #For divergences/distances\n",
    "import scipy #For divergences/distances\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "import sklearn.manifold #For a manifold plot\n",
    "import json #For API responses\n",
    "import urllib.parse #For joining urls\n",
    "\n",
    "# comp-linguistics\n",
    "import spacy\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "import random\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "import lucem_illud_2020\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading NOW raw data\n",
    "\n",
    "The following method iterates through the files in the folder, and unzips the files, storing them in a dictionary with each zip file mapping to a list of the texts.\n",
    "\n",
    "```corpus_name``` is a string which contains the directory of the corpus you need to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"/Users/74068/Documents/Uchicago/courses/Content Analysis/corpus/NOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadcorpus(corpus_name, corpus_style=\"text\"):\n",
    "    texts_raw = {}\n",
    "    for file in os.listdir(corpus_name + \"/\"):\n",
    "        if corpus_style in file:\n",
    "            print(file)\n",
    "            zfile = zipfile.ZipFile(corpus_name + \"/\" + file)\n",
    "            for file in zfile.namelist():\n",
    "                texts_raw[file] = []\n",
    "                with zfile.open(file) as f:\n",
    "                    for line in f:\n",
    "                        texts_raw[file].append(line)                      \n",
    "    return texts_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the movies corpus for our purposes, but you can uncomment the code and try out the other corpora too.\n",
    "You might have to make some adjustments in the cleaning for the other corpora; I have tried it for most of them and it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-16-11.zip\n",
      "text-16-12.zip\n",
      "text-17-01.zip\n",
      "text-17-02.zip\n",
      "text-17-03.zip\n",
      "text-17-04.zip\n",
      "text-17-05.zip\n",
      "text-17-06.zip\n",
      "text-17-07.zip\n",
      "text-17-08.zip\n",
      "text-17-09.zip\n",
      "text-17-10.zip\n",
      "text-17-11.zip\n",
      "text-17-12.zip\n",
      "text-18-01.zip\n",
      "text-18-02.zip\n",
      "text-18-03.zip\n",
      "text-18-04.zip\n",
      "text-18-05.zip\n",
      "text-18-06.zip\n",
      "text-18-07.zip\n",
      "text-18-08.zip\n",
      "text-18-09.zip\n",
      "text-18-10.zip\n"
     ]
    }
   ],
   "source": [
    "now_raw = loadcorpus(corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('now_raw.pkl', 'wb')\n",
    "pickle.dump(now_raw, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read python dict back from the file\n",
    "pkl_file = open('now_raw.pkl', 'rb')\n",
    "now_raw = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('now_raw.txt','wb') as handle:\n",
    "    pickle.dump(now_raw, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-789ca30097b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'now_raw.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnow_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "with open('now_raw.txt','rb') as handle:\n",
    "    now_raw = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"now_raw.txt\",\"w\")\n",
    "f.write(str(now_raw))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_raw=json.load(open(\"now.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(now_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['16-10-au.txt', '16-10-bd.txt', '16-10-ca.txt', '16-10-gb.txt', '18-04-au.txt', '12-05-au.txt', '14-07-au.txt', '10-08-au.txt', '16-03-au.txt', '11-12-au.txt', '12-02-au.txt', '14-08-au.txt', '14-10-au.txt', '18-09-au.txt', '19-07-au.txt', '13-08-au.txt', '16-01-au.txt', '19-08-au.txt', '19-06-au.txt', '18-10-au.txt', 'text_18-01-AU.txt', '13-07-au.txt', '13-09-au.txt', '13-11-au.txt', 'text_17-06-AU.txt', '14-09-au.txt', '12-10-au.txt', '16-07-au.txt', '19-04-au.txt', '12-08-au.txt', 'text_18-02-AU.txt', '14-11-au.txt', '13-03-au.txt', '11-02-au.txt', '13-02-au.txt', '10-09-au.txt', 'text_17-01-AU.txt', '14-12-au.txt', '12-01-au.txt', 'text_16-12-AU.txt', '15-10-au.txt', '11-11-au.txt', '10-10-au.txt', '11-03-au.txt', '10-04-au.txt', '12-04-au.txt', '13-05-au.txt', '15-09-au.txt', '12-09-au.txt', 'text_17-09-AU.txt', '16-04-au.txt', '12-06-au.txt', '14-05-au.txt', '19-09-au.txt', 'text_17-02-AU.txt', '15-02-au.txt', '10-11-au.txt', '12-11-au.txt', '18-05-au.txt', '12-03-au.txt', '15-12-au.txt', '11-05-au.txt', '13-01-au.txt', '10-07-au.txt', '19-05-au.txt', 'text_17-05-AU.txt', '10-03-au.txt', '11-10-au.txt', '13-06-au.txt', '11-07-au.txt', 'text_17-10-AU.txt', '14-01-au.txt', '16-06-au.txt', '11-04-au.txt', 'text_17-07-AU.txt', '19-03-au.txt', '11-09-au.txt', '18-11-au.txt', '19-02-au.txt', '15-04-au.txt', 'text_16-11-AU.txt', '11-06-au.txt', '18-06-au.txt', '13-12-au.txt', '10-01-au.txt', '12-07-au.txt', 'text_17-11-AU.txt', '14-03-au.txt', '10-02-au.txt', '16-02-au.txt', '19-01-au.txt', '15-03-au.txt', '16-05-au.txt', 'text_18-03-AU.txt', 'text_17-12-AU.txt', '15-05-au.txt', '15-07-au.txt', '13-04-au.txt', '15-08-au.txt', '18-07-au.txt', '16-09-au.txt', '15-06-au.txt', '18-08-au.txt', 'text_17-08-AU.txt', '15-01-au.txt', '10-05-au.txt', 'text_17-03-AU.txt', '14-02-au.txt', '12-12-au.txt', '14-06-au.txt', '10-12-au.txt', '18-12-au.txt', '13-10-au.txt', '11-01-au.txt', 'text_17-04-AU.txt', '14-04-au.txt', '16-08-au.txt', '15-11-au.txt', '11-08-au.txt', '10-06-au.txt'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'18-11-hk.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-508a1d512c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnow_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'18-11-hk.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '18-11-hk.txt'"
     ]
    }
   ],
   "source": [
    "now_raw['18-11-hk.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16-10-bd.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(now_raw.keys())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems messy, but nothing we can't clean. This basic method replaces some of the issues with the formatting, and prints the errors if any for debugging. Let us clean one of the raw text files. \n",
    "\n",
    "Note: we skip any text data which isn't utf-8 encoded here. I do this to keep things clean; you might want more data or anticipate special characters and not include that restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_text(raw_texts):\n",
    "    clean_texts = []\n",
    "    for text in raw_texts:\n",
    "        try:\n",
    "            text = text.decode(\"utf-8\")\n",
    "            clean_text = text.replace(\" \\'m\", \"'m\").replace(\" \\'ll\", \"'ll\").replace(\" \\'re\", \"'re\").replace(\" \\'s\", \"'s\").replace(\" \\'re\", \"'re\").replace(\" n\\'t\", \"n't\").replace(\" \\'ve\", \"'ve\").replace(\" /'d\", \"'d\")\n",
    "            clean_texts.append(clean_text)\n",
    "        except AttributeError:\n",
    "            # print(\"ERROR CLEANING\")\n",
    "            # print(text)\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            # print(\"Unicode Error, Skip\")\n",
    "            continue\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'18-11-hk.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d25ca75bbff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_raw_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'18-11-hk.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclean_11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '18-11-hk.txt'"
     ]
    }
   ],
   "source": [
    "clean_11 = clean_raw_text(now_raw['18-11-hk.txt'])\n",
    "clean_11[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. This is looking a lot cleaner. We can now run some of our lucem_illud text cleaning methods we discuss/model in week 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    \n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(clean_11[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeTokens(clean_11[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let us create a Pandas dataframe with movie names, raw words, tokenized words, and so on.\n",
    "The file \"sources_movies.zip\" has this information. Similar information files are found for the other datasets too, in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadsouce(corpus_name, corpus_style=\"source\"):\n",
    "    source = []\n",
    "    for file in os.listdir(corpus_name + \"/\"):\n",
    "        if corpus_style in file:\n",
    "            print(file)\n",
    "            zfile = zipfile.ZipFile(corpus_name + \"/\" + file)\n",
    "            for file in zfile.namelist():\n",
    "                with zfile.open(file) as f:\n",
    "                    for line in f:\n",
    "                        source.append(line)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_source=loadsouce(corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "now_source[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks dirty because the file is encoded as bytes, but we can certainly see the information in here. The file id is also present in the original raw text data: as the first \"word\". Look back at the normalized/tokenized words to confirm that. We're going to use this to create a dataframe with: Fileid, movie name, genre, year, and country.\n",
    "\n",
    "It is advised that you run a similar check of the source file before you do other extraction.\n",
    "\n",
    "First, let us create a dictionary mapping file-id to all the text. Each movie will be mapped to a list of the tokenized words.\n",
    "\n",
    "In this example, I only use it to load 1000 movies. You can comment this out or increase/decrease the number as inspired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_texts={}\n",
    "now_df = pd.DataFrame(columns=[\"filename\", \"Year\", \"Month\", \"Country\", \"id\", \"Tokenized Text\", \"Normalized Text\", \"Raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in now_raw:\n",
    "    \n",
    "    if len(now_texts)>48000:\n",
    "        break\n",
    "        \n",
    "    print(files)\n",
    "    # nows = clean_raw_text(now_raw[files][1:])\n",
    "    if len(now_raw[files]) > 20:\n",
    "        n=random.sample(list(now_raw[files]),20)\n",
    "    else:\n",
    "        n=random.sample(list(now_raw[files]),5)\n",
    "    nows = clean_raw_text(n)\n",
    "    for now in nows:\n",
    "        txts_tokenized = word_tokenize(now)\n",
    "        txts_nomalized = normalizeTokens(txts_tokenized)\n",
    "        try:\n",
    "            now_df=now_df.append({'filename': files,'id': txts_nomalized[0], 'Tokenized Text': txts_tokenized[4:], 'Normalized Text':txts_nomalized[4:],'Raw':now},ignore_index=True)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            now_texts[txts_nomalized[0][2:]] = txts_nomalized[1:]\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in now_raw[0:5]:\n",
    "    for file in files[0:3]:\n",
    "        print(\"a\"+ file)\n",
    "        if \"technology\" in file:\n",
    "            print(\"b\"+ file)\n",
    "            txts_tokenized = word_tokenize(file)\n",
    "            txts_nomalized = normalizeTokens(file)\n",
    "            try:\n",
    "                now_df=now_df.append({'filename': files,'id': txts_nomalized[0], 'Tokenized Text': txts_tokenized[4:], 'Normalized Text':txts_nomalized[4:],'Raw':file},ignore_index=True)\n",
    "            except IndexError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(now_raw['10-08-ke.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_df.to_csv('now.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.DataFrame(columns=[\"Date\", \"Country\", \"Media\", \"Link\", \"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for now in now_source[0:]:\n",
    "    print(now)\n",
    "    try:\n",
    "        tid, fileid, date, country, media, link, title = now.decode(\"utf-8\").split(\"\\t\")\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    try:\n",
    "        source_df.loc[tid.strip()] = [date.strip(), country.strip(), media.strip(), link.strip(), title.strip()]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.to_csv('source.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains information of the name, the genre, the year, the country, and the texts associated with it: all sorts of analysis can be run with this information now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try the similar process and load the other datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
